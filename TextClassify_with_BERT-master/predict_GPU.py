 # -*- coding: utf-8 -*-
'''
@author: yaleimeng@sina.com
@license: (C) Copyright 2019
@desc: è¿™ä¸ªä»£ç æ˜¯è¿›è¡Œé¢„æµ‹çš„ã€‚æ—¢å¯ä»¥æ ¹æ®ckptæ£€æŸ¥ç‚¹ï¼Œä¹Ÿå¯ä»¥æ ¹æ®å•ä¸ªpbæ¨¡å‹ã€‚
@DateTime: Created on 2019/7/19, at ä¸‹åˆ 04:13 by PyCharm
'''
from train_eval import *
from tensorflow.python.estimator.model_fn import EstimatorSpec


class Bert_Class():

    def __init__(self):
        self.graph_path = os.path.join(arg_dic['pb_model_dir'], 'classification_model.pb')
        self.ckpt_tool, self.pbTool = None, None
        self.prepare()

    def classification_model_fn(self, features, mode):
        with tf.gfile.GFile(self.graph_path, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())
        input_ids = features["input_ids"]
        input_mask = features["input_mask"]
        input_map = {"input_ids": input_ids, "input_mask": input_mask}
        pred_probs = tf.import_graph_def(graph_def, name='', input_map=input_map, return_elements=['pred_prob:0'])

        return EstimatorSpec(mode=mode, predictions={
            'encodes': tf.argmax(pred_probs[0], axis=-1),
            'score': tf.reduce_max(pred_probs[0], axis=-1)})

    def prepare(self):
        tokenization.validate_case_matches_checkpoint(arg_dic['do_lower_case'], arg_dic['init_checkpoint'])
        self.config = modeling.BertConfig.from_json_file(arg_dic['bert_config_file'])

        if arg_dic['max_seq_length'] > self.config.max_position_embeddings:
            raise ValueError(
                "Cannot use sequence length %d because the BERT model "
                "was only trained up to sequence length %d" %
                (arg_dic['max_seq_length'], self.config.max_position_embeddings))

        # tf.gfile.MakeDirs(self.out_dir)
        self.tokenizer = tokenization.FullTokenizer(vocab_file=arg_dic['vocab_file'],
                                                    do_lower_case=arg_dic['do_lower_case'])

        self.processor = SelfProcessor()
        self.train_examples = self.processor.get_train_examples(arg_dic['data_dir'])
        global label_list
        label_list = self.processor.get_labels()

        self.run_config = tf.estimator.RunConfig(
            model_dir=arg_dic['output_dir'], save_checkpoints_steps=arg_dic['save_checkpoints_steps'],
            tf_random_seed=None, save_summary_steps=100, session_config=None, keep_checkpoint_max=5,
            keep_checkpoint_every_n_hours=10000, log_step_count_steps=100, )

    def predict_on_ckpt(self, sentence):
        if not self.ckpt_tool:
            num_train_steps = int(len(self.train_examples) / arg_dic['train_batch_size'] * arg_dic['num_train_epochs'])
            num_warmup_steps = int(num_train_steps * arg_dic['warmup_proportion'])

            model_fn = model_fn_builder(bert_config=self.config, num_labels=len(label_list),
                                        init_checkpoint=arg_dic['init_checkpoint'], learning_rate=arg_dic['learning_rate'],
                                        num_train=num_train_steps, num_warmup=num_warmup_steps)

            self.ckpt_tool = tf.estimator.Estimator(model_fn=model_fn, config=self.run_config, )
        exam = self.processor.one_example(sentence)  # å¾…é¢„æµ‹çš„æ ·æœ¬åˆ—è¡¨
        feature = convert_single_example(0, exam, label_list, arg_dic['max_seq_length'], self.tokenizer)

        predict_input_fn = input_fn_builder(features=[feature, ],
                                            seq_length=arg_dic['max_seq_length'], is_training=False,
                                            drop_remainder=False)
        result = self.ckpt_tool.predict(input_fn=predict_input_fn)  # æ‰§è¡Œé¢„æµ‹æ“ä½œï¼Œå¾—åˆ°ä¸€ä¸ªç”Ÿæˆå™¨ã€‚
        gailv = list(result)[0]["probabilities"].tolist()
        pos = gailv.index(max(gailv))  # å®šä½åˆ°æœ€å¤§æ¦‚ç‡å€¼ç´¢å¼•ï¼Œ
        return label_list[pos]

    def predict_on_pb(self, sentence):
        if not self.pbTool:
            self.pbTool = tf.estimator.Estimator(model_fn=self.classification_model_fn, config=self.run_config, )
        exam = self.processor.one_example(sentence)  # å¾…é¢„æµ‹çš„æ ·æœ¬åˆ—è¡¨
        feature = convert_single_example(0, exam, label_list, arg_dic['max_seq_length'], self.tokenizer)
        predict_input_fn = input_fn_builder(features=[feature, ],
                                            seq_length=arg_dic['max_seq_length'], is_training=False,
                                            drop_remainder=False)
        result = self.pbTool.predict(input_fn=predict_input_fn)  # æ‰§è¡Œé¢„æµ‹æ“ä½œï¼Œå¾—åˆ°ä¸€ä¸ªç”Ÿæˆå™¨ã€‚
        ele = list(result)[0]
        print('ç±»åˆ«ï¼š{}ï¼Œç½®ä¿¡åº¦ï¼š{:.3f}'.format(label_list[ele['encodes']], ele['score']))
        return label_list[ele['encodes']]


if __name__ == "__main__":
    import time

    testcase = ['plog ç”Ÿæ´»ç¢ç‰‡ æ˜¥å¤©çœŸçš„å…»çœ¼ç›ï¼Œå•ä½é‡Œçš„çš„ç»¿åŒ–è®©äººçœ‹äº†å¿ƒæ—·ç¥æ€¡ï¼Œæ¯å¤©åˆé¥­åéƒ½è¦å»èµ°ä¸€åœˆï¼Œå‘¨æœ«ä¸€å®¶äººå»å°„ç®­ğŸ¹äº†ï¼ŒæŒºå¥½ç©çš„', 'ä»Šå¤©çœŸå€’éœ‰ï¼Œè¢«åº—é•¿è¯´äº†ï¼Œè¿˜ä¸å°å¿ƒæŠŠåˆ«äººçš„è½¦å¼„å€’äº† ï¼›ä»Šå¤©è«åå…¶å¦™çš„è¢«åŒäº‹éª‚äº†ï¼Œéª‚çš„å¾ˆè„ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“æˆ‘åšé”™äº†ä»€ä¹ˆ ï¼›æ€ä¹ˆåŠï¼Œæˆ‘è¢«åº—é•¿è¯´äº†ï¼šè¦çœ¼é‡Œæœ‰æ´»ï¼Œä¸è¦å‘å‘†ï¼Œè¯´è¯è¦æœ‰å›åº”ï¼Œå¯æˆ‘å°±æ˜¯ä¸çˆ±è¯´è¯å•Š ï¼Œæ€ä¹ˆåŠæˆ‘æ˜¯ä¸æ˜¯è¦è¢«å¼€é™¤äº† ï¼›å¥¶èŒ¶å¥½è…»å•Šï¼Œå†ä¹Ÿä¸åƒäº† ï¼›åˆæ˜¯emoçš„æ™šä¸Š ï¼›ä»Šå¤©å·¥ä½œåˆèƒ¡æ€ä¹±æƒ³äº†']
    toy = Bert_Class()
    aaa = time.clock()
    for t in testcase:
        print(toy.predict_on_ckpt(t), t)
    bbb = time.clock()
    print('ckpté¢„æµ‹ç”¨æ—¶ï¼š', bbb - aaa)

    aaa = time.clock()
    for t in testcase:
        toy.predict_on_pb(t)
    bbb = time.clock()
    print('pbæ¨¡å‹é¢„æµ‹ç”¨æ—¶ï¼š', bbb - aaa)
